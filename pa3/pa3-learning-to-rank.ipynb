{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PA3 - Learning to Rank (52% of total PA3 grade)\n",
    "\n",
    "In the first part of this assignment, we examined various ways of ranking documents given a query; however, weights for different features were not learned automatically but set manually. As more and more ranking signals are investigated, integrating more features becomes challenging as it would be hard to come up with a single ranking function like BM25 for arbitrary features. \n",
    "\n",
    "In this assignment, you will be investigating different approaches to the learning to rank task that you have learned: (1) the pointwise approach using linear regression and (2) the pairwise approach employing gradient boosted decision trees. The goal is to let these algorithms learn weights automatically for various features. \n",
    "\n",
    "More specifically, it involves the following tasks (weights are for the programming assignment as a whole):\n",
    "* [Task 1: Pointwise Approach and Linear Regression (6%)](#Task-1:-Pointwise-Approach-and-Linear-Regression): Implement an instance of the pointwise approach with linear regression based on basic tf-idf features\n",
    "* [Task 2: Pairwise Approach and Gradient Boosted Decision Trees (6%)](#Task-2:-Pairwise-Approach-and-Gradient-Boosted-Decision-Trees): Implement an instance of the pairwise approach with the help of gradient boosted decision trees, using basic tf-idf features\n",
    "* [Task 3: Train Your Best Model (20%)](#Task-3:-Adding-More-Features) Train your best model, and experiment with more features such as BM25, Smallest Window, and PageRank\n",
    "* [Task 4: Report (20%)](#Task-4:-Report): Write up a summary report and answer some questions about the above tasks\n",
    "* [Extra Credit](#Extra-Credit): Up to 10% in extra credit will be awarded to the top performing models in the class\n",
    "\n",
    "(Note: 3% of your grade on this programming assignment is reserved for completing the query and ranking quizzes). \n",
    "\n",
    "__Grading for Tasks 1, 2 and 3__\n",
    "- Half of your grade will be based on your model's performance on an autograder test set. Your scores will be visible to you when you submit on Gradescope, but the test set will not. \n",
    "- The other half of your grade will be based on your model's performance on a hidden test set. Your scores will only be visible to you when grades for this assignment are released\n",
    "- You will get full credit for solutions that receive NDCG scores within reasonable range of the NDCG scores received by the teaching staff."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission instructions\n",
    "\n",
    "1\\. The assignment is due at 4:00 pm PT on the due date (30 May, 2019)\n",
    "\n",
    "2\\. The notebook will automatically generate **python files** in submission folder. You'll have to upload them to the PA3-code assignment on gradescope. Note that you need to upload all the individual files in the submission folder without zipping it.    \n",
    "\n",
    "3\\. While solving the assignment, do **NOT** change class and method names, autograder tests will fail otherwise. \n",
    "\n",
    "4\\. You'll also have to upload a **PDF version** of the notebook (which would be primarily used to grade your report section of the notebook) to PA3-PDF assignment on gradescope. Note that directly converting the PDF truncates code cells. To get a usable PDF version, first click on File > Print Preview, which will open in a new tab, then print to PDF using your browser's print functionality. \n",
    "\n",
    "5\\. Since there are two notebooks, we have included a script to help you merge them together before upload. Run\n",
    "```\n",
    "python pdfcat pa3-ranking.pdf pa3-learning-to-rank.pdf > pa3-solution.pdf\n",
    "``` \n",
    "to generate a single concatenated pdf file and upload `pa3-solution.pdf` to gradescope.\n",
    "\n",
    "6\\. After uploading the PDF make sure you **tag all the relevant pages to each question**. We will penalize for mistagged submissions. \n",
    "\n",
    "7\\. If you are solving the assignment in a team of two, add the other student as a group member after submitting the assignment. Do **NOT** submit the same assignment twice. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the tee magic which saves a copy of the cell when executed\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%reload_ext autograding_magics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `submission` folder will contain all the files to be submitted, and `base_classes` contains other class definitions which you will not submit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "try: \n",
    "    os.mkdir('submission')\n",
    "except FileExistsError:\n",
    "    pass\n",
    "try: \n",
    "    os.mkdir('base_classes')\n",
    "except FileExistsError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can add additional imports below as required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%tee submission/imports2.py\n",
    "\n",
    "# You can add additional imports here\n",
    "\n",
    "import sys\n",
    "import pickle as pkl\n",
    "import array\n",
    "import os\n",
    "import timeit\n",
    "import contextlib\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from collections import Counter\n",
    "from collections import OrderedDict\n",
    "import math\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "from base_classes.load_train_data import load_train_data\n",
    "from base_classes.id_map import IdMap\n",
    "from base_classes.ndcg import NDCG\n",
    "from base_classes.query import Query\n",
    "from base_classes.document import Document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This dataset is the same as what you used in the first part of this Programming Assignment. You do not need to download it again.**\n",
    "\n",
    "As in the first part of this programming assignment, we have partitioned the data into two sets for you: \n",
    "1. Training set (pa3.(signal|rel).train)\n",
    "2. Development set (pa3.(signal|rel).dev)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading previous code\n",
    "\n",
    "We load the AScorer class that you completed in the first part of Programming Assignment 3. Note that you may need to make updates to this class for completing the tasks in this notebook.\n",
    "\n",
    "We also load the Idf class that you can use to get document frequency values based on a corpus of ~100K documents and ~340K terms. You will also need to load the Rank class for the computation of NDCG scores on the tasks below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from submission.ascore import AScorer\n",
    "from submission.build_idf import Idf\n",
    "from submission.rank import Rank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1: Pointwise Approach and Linear Regression (6%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In ranking, each query $q_i$ will be associated with a set of documents, and for each document $j$, we extract a query-document feature vector $x_{i,j}$. There is also a label $y_{i,j}$ associated with each query-document vector $x_{i,j}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the pointwise approach, such group structure in ranking is ignored, and we simply view our training data as $\\{(x_{i}, y_{i})\\}$, where each instance consists of a query-document feature vector $x_{i}$ and a label $y_{i}$ (which is a relevance score as in the first part of this programming assignment). The ranking problem amounts to learning a function $f$ such that $f(x_{i})$ closely matches $y_{i}$.\n",
    "\n",
    "In this task, we consider a very simple instance of the pointwise approach, the *linear regression* approach. That is, we will use a linear function $f$ which gives a score to each query-document feature vector $x$ as follows: $f(x) = wx+b$. Here, the weight vector ${w}$ and the bias term $b$ are parameters that we need to learn to minimize the loss function as defined below:\n",
    "\\begin{equation}\n",
    "\\sum_{i=1}^m (f(x_{i})-y_{i})^2\n",
    "\\end{equation}\n",
    "This formulation is also referred to as the *ordinary least squares* approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1: Designing Feature Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Represent each query-document pair as a five-dimensional vector of query vector-document vector (tf-idf) scores. Each dimension corresponds to a document field -- url, title, header, body, and anchor. Specifically, given a query vector $q$ and a document vector $d_{f}$ of a document field $f$, the tf-idf score is the dot product $q \\cdot d_{f}$. \n",
    "\n",
    "To start with, use query and document vectors with lnn.ltc weighting (as represented in SMART notation ddd.qqq). In other words, begin by using:\n",
    "\n",
    "1) For the document vectors, \"lnn\":\n",
    "    - logarithmic term frequency of query terms in documents\n",
    "    - no document frequency \n",
    "    - no normalization\n",
    "2) For the query vector, \"ltc\":\n",
    "    - logarithmic term frequency for words in query\n",
    "    - idf (inverse document frequency)\n",
    "    - cosine (i.e., L2) normalization\n",
    "    \n",
    "Then, experiment with a few weighting schemes other than lnn.ltc.  Refer to http://web.stanford.edu/class/cs276/19handouts/lecture6-tfidf-1per.pdf for other possible weighting schemes. You will report which weighting scheme yields the best performance in Task 4.\n",
    "\n",
    "A few important notes:\n",
    "- Creating these vectors is similar to the exercise you performed in computing cosine similarity in the first part of this programming assignment\n",
    "- Make modifications to the AScorer class in order to try to implement other weighting mechanisms \n",
    "- **You will use these basic feature vectors for both Task 1 and Task 2. Do not use any other signals or features for Tasks 1 and 2; you will have the opportunity to use these features in Task 3.**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%tee submission/features.py\n",
    "\n",
    "def get_features (signal_file, idf):\n",
    "    '''\n",
    "    Create a feature vector from the signal file and from the idf_dict. \n",
    "\n",
    "    Args:\n",
    "        signal_file: filepath to signal file\n",
    "        idf: object of class Idf (with idf built)\n",
    "\n",
    "    Returns:\n",
    "        feature_vec (numpy array of dimension (N, 5)): N is the number of (query, document)\n",
    "        pairs in the relevance file.\n",
    "    '''\n",
    "\n",
    "    # Experiment with different values of weighting below. Note that this uses dddqqq notation.\n",
    "    # Make sure to set weighting to the best value prior to submitting your code.\n",
    "    # You should be able to support lnn.ltc weighting, along with any other weighting that you experiment with\n",
    "\n",
    "\n",
    "    WEIGHTING = 'lnnltc' \n",
    "\n",
    "    assert len(WEIGHTING) == 6, \"Invalid weighting scheme.\"        \n",
    "\n",
    "    feature_vec = []\n",
    "\n",
    "    ### Begin your code\n",
    "\n",
    "    ### End your code\n",
    "\n",
    "    return feature_vec\n",
    "\n",
    "\n",
    "def get_relevance (relevance_file):\n",
    "    '''\n",
    "    Extract relevance scores from the relevance file. This should be a simple wrapper (<10 lines) over\n",
    "    the get_rel_scores() function in the NDCG class.\n",
    "\n",
    "    Args:\n",
    "        relevance_file: filepath to relevance file\n",
    "\n",
    "    Returns:\n",
    "        relevance_vec (numpy array of dimension (N,)): N is the number of (query, document)\n",
    "        pairs in the relevance file.   \n",
    "        ndcg_obj: NDCG object which contains relevance scores\n",
    "    '''  \n",
    "\n",
    "\n",
    "    relevance_vec = []\n",
    "    ndcg_obj = NDCG()\n",
    "\n",
    "    ### Begin your code\n",
    "\n",
    "    ### End your code\n",
    "\n",
    "    return relevance_vec, ndcg_obj\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "   \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2: Training a Linear Regression Model\n",
    "\n",
    "Implement the PointwiseLearner class below. You may use the LinearRegression class from the sklearn package. If you use the LinearRegression class, set fit_intercept to true and normalize to False."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%tee submission/linear_regression.py\n",
    "\n",
    "class PointwiseLearner:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.model = None\n",
    "\n",
    "    def train_model (self, x, y):\n",
    "    \n",
    "        '''\n",
    "        - Train your linear regression model using the LinearRegression class \n",
    "\n",
    "        Args:\n",
    "                x (numpy array of dimension (N, 5)): Feature vector for each query, document pair. \n",
    "                Dimension is N x 5, where N is the number of query, document pairs. \n",
    "                Is the independent variable for linear regression. \n",
    "\n",
    "                y (numpy array of dimension (N,)): Relevance score for each query, document pair. \n",
    "                Is the dependent variable for linear regresion.\n",
    "\n",
    "        Returns: none\n",
    "        '''\n",
    "        ### Begin your code\n",
    "\n",
    "        ### End your code\n",
    "    \n",
    "    def predict_model (self, x):\n",
    "    \n",
    "        '''\n",
    "        - Output predicted scores based on the trained model.\n",
    "\n",
    "        Args:\n",
    "                x (numpy array of dimension (N, 5)): Feature vector for each query, document pair. \n",
    "                Dimension is N x 5, where N is the number of (query, document) pairs. \n",
    "                Predictions are made on this input feature array.\n",
    "\n",
    "        Returns:\n",
    "                y_pred (numpy array of dimension (N,)): Predicted relevance scores for each query, document pair\n",
    "                based on the trained linear regression model.\n",
    "        '''\n",
    "        ### Begin your code\n",
    "\n",
    "        ### End your code\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = PointwiseLearner()\n",
    "\n",
    "idf = Idf()\n",
    "\n",
    "#Get train features and relevance\n",
    "\n",
    "train_signal_file = \"pa3-data/pa3.signal.train\"\n",
    "train_rel_file = \"pa3-data/pa3.rel.train\"\n",
    "train_features = get_features(train_signal_file, idf)\n",
    "train_relevance, train_ndcg = get_relevance(train_rel_file)\n",
    "assert train_features.shape[1] == 5, 'Train features are of incorrect shape. They should be 5 dimensions, but got {}'.format(train_predicts.shape[1])\n",
    "\n",
    "#Train linear regression model\n",
    "\n",
    "lm.train_model(train_features, train_relevance)\n",
    "\n",
    "# Get predictions on dev set.\n",
    "dev_signal_file = \"pa3-data/pa3.signal.dev\"\n",
    "dev_rel_file = \"pa3-data/pa3.rel.dev\"\n",
    "dev_features = get_features(dev_signal_file, idf)\n",
    "dev_relevance, dev_ndcg =  get_relevance(dev_rel_file)\n",
    "dev_predicts = lm.predict_model(dev_features)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure your code passes the sanity check below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert dev_features.shape[1] == 5, 'Train features are of incorrect shape. They should be 5 dimensions, but got {}'.format(train_predicts.shape[1])\n",
    "\n",
    "assert dev_relevance.shape[0]== 1187, 'Relevance vector is of incorrect shape. Expected 1187, but got {}'.format(dev_relevance.shape[0])\n",
    "\n",
    "assert dev_predicts.shape[0] == 1187, 'Predictions are of incorrect shape. Expected 1187, but got {}'.format(dev_predicts.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "Using the predictions from your trained model, compute the mean squared error and NDCG score that you receive.  Include the score you received in your report. For the development data set, the course staff received an NDCG score of ~0.83."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NDCG_calc_for_LTR (dev_ndcg, dev_predicts, out_file=\"ranked_result_default\"):\n",
    "\n",
    "    ''' We provide this function to calculate the average NDCG score given a predicted score and a ground truth score.\n",
    "        Note that the code below calls rank_with_score() in the Rank class, so the correct value for NDCG \n",
    "        depends on the correct implmementation of that function.\n",
    "         Args:\n",
    "                dev_ndcg (type NDCG): Object that contains the \"ground truth\" relevance scores in dev_ndcg.rel_scores \n",
    "                dev_predicts: numpy array of dimension (N,) which contains predicted scores for a dataset.\n",
    "                out_file: filename to which the ranked_result_file is written\n",
    "            \n",
    "        Returns: avg_ndcg_score: Scalar that averages NDCG score across all queries. \n",
    "    \n",
    "    '''\n",
    "    idx = 0\n",
    "    dev_predicts_dict = {}\n",
    "\n",
    "    #Converts the dev_predicts vector into query->url->score dict\n",
    "    for query, url_dict in dev_ndcg.rel_scores.items():\n",
    "        query_obj = Query(query) #Converts str to Query object\n",
    "        dev_curr_dict = {}\n",
    "        for url in url_dict.keys():\n",
    "            dev_curr_dict[url] = dev_predicts[idx]\n",
    "            idx+=1\n",
    "        dev_predicts_dict[query_obj] = dev_curr_dict\n",
    "\n",
    "    #Orders dev_predicts_dict. This remains a Query->url->score dict after ordering.\n",
    "    #Note that this depends on your implementation of the rank_with_score() function in the Rank class.\n",
    "    r = Rank()\n",
    "    dev_predicts_dict_ordered = r.rank_with_score(dev_predicts_dict)\n",
    "\n",
    "    #Creates a Query->Document->score dict called dev_predicts_ranks that will be written to file.\n",
    "    dev_data = load_train_data(dev_signal_file) #Query->Document dict\n",
    "\n",
    "    dev_predicts_ranked = {} #The Query->Document->Score dict that will be written to file.\n",
    "    for query in dev_predicts_dict_ordered:\n",
    "        doc_to_score = {}\n",
    "        for url in dev_predicts_dict_ordered[query]:\n",
    "            doc = dev_data[query][url]\n",
    "            doc_to_score[doc] = dev_predicts_dict_ordered[query][url]\n",
    "        dev_predicts_ranked[query] = doc_to_score\n",
    "\n",
    "    #Writes dev_predicts_ranked to file.\n",
    "    ranked_result_file = os.path.join(\"output\", out_file)\n",
    "    r.write_ranking_to_file(dev_predicts_ranked, ranked_result_file)\n",
    "\n",
    "    #Uses the NDCG class to get the NDCG score\n",
    "    dev_ndcg.read_ranking_calc(ranked_result_file)\n",
    "    avg_ndcg_score = dev_ndcg.get_avg_ndcg()\n",
    "    return avg_ndcg_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute mean squared error and NDCG Score\n",
    "\n",
    "mse = mean_squared_error(dev_relevance, dev_predicts)\n",
    "\n",
    "print (\"Mean Squared Error:\", mse)\n",
    "\n",
    "print (\"Average NDCG score:\", NDCG_calc_for_LTR(dev_ndcg, dev_predicts, \"ranked_result_pointwise\"))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: Gradient Boosted Decision Trees (6%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We next use the LambdaMART algorithm to implement Gradient Boosted Decision Trees. \n",
    "\n",
    "LambdaMART is the boosted tree version of an earlier algorithm, LambdaRank. The full evolution of algorithms from RankNet through LambdaRANK, MART and LambdaMART is presented below (Page 16 and 17 are particularly important). \n",
    "https://pdfs.semanticscholar.org/0df9/c70875783a73ce1e933079f328e8cf5e9ea2.pdf\n",
    "\n",
    "The relevant lecture notes can be found here: http://web.stanford.edu/class/cs276/19handouts/lecture15-learning-ranking-1per.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the XGBoost package to implement LambdaMART. You may find it helpful to read the documentation here: https://xgboost.readthedocs.io/en/latest/get_started.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parameter description (not exhaustive, see here for more details): https://xgboost.readthedocs.io/en/latest/parameter.html\n",
    "\n",
    "General Parameters (**make sure to use the following values**):\n",
    "- \"booster\": use \"gbtree\". Uses a tree-based model for boosting\n",
    "- \"objective\": use \"rank:pairwise\". Uses the LambdaMART algorithm to minimize pairwise loss. \n",
    "- \"eval_metric: use \"ndcg\" (while we will be evaluating your performance solely based on ndcg, feel free to test performance on other metrics)\n",
    "\n",
    "Hyperparamters to be tuned (not exhaustive):\n",
    "- \"eta\": Learning rate\n",
    "- \"gamma\": Minimum loss reduction required to make a further partition on a leaf node of the tree\n",
    "- \"max_depth\": Maximum depth of a tree\n",
    "- \"subsample\": Subsample ratio of training instances to prevent overfitting\n",
    "\n",
    "When training, you should also experiment with early stopping to prevent overfitting. Take a look at the description of early stopping here: https://xgboost.readthedocs.io/en/latest/python/python_intro.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_query_dict = load_train_data(train_signal_file)\n",
    "train_groups = []\n",
    "for query, url_dict in train_query_dict.items():\n",
    "    train_groups.append(len(url_dict))\n",
    "    \n",
    "assert sum(train_groups) == 7026, 'Expected 7026 (query, doc) pairs, but got {}'.format(sum(train_groups))\n",
    "assert len(train_groups) == 731, 'Expected 731 queries, but got {}'.format(len(train_groups))\n",
    "\n",
    "\n",
    "dev_query_dict = load_train_data(dev_signal_file)\n",
    "dev_groups = []\n",
    "for query, url_dict in dev_query_dict.items():\n",
    "    dev_groups.append(len(url_dict))\n",
    "    \n",
    "assert sum(dev_groups) == 1187, 'Expected 1187 (query, doc) pairs, but got {}'.format(sum(train_groups))\n",
    "assert len(dev_groups) == 124, 'Expected 124 queries, but got {}'.format(len(train_groups))\n",
    "\n",
    "dtrain = xgb.DMatrix(train_features, label = train_relevance)\n",
    "dtrain.set_group(train_groups)\n",
    "ddev = xgb.DMatrix(dev_features, label = dev_relevance) \n",
    "ddev.set_group(dev_groups)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%tee submission/gbdt.py\n",
    "\n",
    "class GBDTLearner:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.params = None\n",
    "        self.model = None\n",
    "\n",
    "    def train_model (self, dtrain, evallist):\n",
    "    \n",
    "        '''\n",
    "        - Specifies parameters for XGBoost training\n",
    "        - Trains model\n",
    "\n",
    "        Args:\n",
    "                dtrain (type DMatrix): DMatrix is a internal data structure that used by XGBoost \n",
    "                which is optimized for both memory efficiency and training speed.\n",
    "                \n",
    "                evallist (array of tuples): The datasets on which the algorithm reports performance as training takes place\n",
    "                \n",
    "\n",
    "        Returns: none\n",
    "        '''\n",
    "        num_rounds = 10 #Experiment with different values of this parameter\n",
    "        \n",
    "        ### Begin your code\n",
    "\n",
    "        ### End your code\n",
    "    \n",
    "    def predict_model (self, dtest):\n",
    "    \n",
    "        '''\n",
    "        - Output predicted scores based on the trained model.\n",
    "\n",
    "        Args:\n",
    "                dtest (type DMatrix): DMatrix that contains the dev/test signal data\n",
    "\n",
    "        Returns:\n",
    "                y_pred (numpy array of dimension (N,)): Predicted relevance scores for each query, document pair\n",
    "                based on the trained  model.\n",
    "        '''\n",
    "        ### Begin your code\n",
    "\n",
    "        ### End your code\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train a gradient boosted decision trees model.\n",
    "\n",
    "model = GBDTLearner()\n",
    "evallist = [(dtrain, 'train')]\n",
    "model.train_model(dtrain, evallist)\n",
    "\n",
    "# Get predictions on dev set.\n",
    "\n",
    "dev_predicts_gbdt = model.predict_model(ddev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert dev_predicts_gbdt.shape[0] == 1187, 'Predictions are of incorrect shape. Expected 1187, but got {}'.format(dev_predicts.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"Average NDCG score:\", NDCG_calc_for_LTR(dev_ndcg, dev_predicts_gbdt, \"ranked_result_gbdt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3: Train your best model (20%)\n",
    "\n",
    "Putting it all together! In this part, train your best model - and feel free to use additional features! Experiment with the following to see which yields the best performance on the dev set:\n",
    "\n",
    "1. Using smallest window feature from the first part of this programming assignment\n",
    "2. Using BM-25 from the first part of this programming assignment\n",
    "3. Using Pagerank from the idf file\n",
    "\n",
    "In addition, you may also choose to experiment with using word vectors. We provide GLoVE embeddings for the words in our vocabulary, which you can download with the help of embedding.py in the base_classes folder.\n",
    "\n",
    "The grader will interface exclusively with the train_and_predict function, but you may choose to write several helper functions as required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%tee submission/best_model.py\n",
    "\n",
    "class BestModel:\n",
    "    \n",
    "    def __init__(self):\n",
    "    ### Begin your code\n",
    "\n",
    "    ### End your code\n",
    "   \n",
    "    # You may choose to write other helper functions below \n",
    "    # (such as to augment feature array with additional features)\n",
    "    \n",
    "    ### Begin your code\n",
    "\n",
    "    ### End your code\n",
    "    \n",
    "    \n",
    "    def train_and_predict(self, train_signal_file, train_rel_file, test_signal_file, idf):\n",
    "    \n",
    "        '''\n",
    "        - Receives the training signal and relevance files as parameters\n",
    "        - Creates a feature vector associated with the signal file\n",
    "        - Trains the best possible model on the training data\n",
    "        - Using the trained model, makes a prediction on the test_signal_file\n",
    "        \n",
    "        - \n",
    "\n",
    "        Args:\n",
    "            train_signal_file: filename of training signal\n",
    "            train_rel_file: filename of training relevance file\n",
    "            test_signal_file: filename containing dev/test signal\n",
    "            idf: object of class IDF, containing a fully built idf dictionary\n",
    "            \n",
    "\n",
    "        Returns: none\n",
    "        '''\n",
    "        test_predictions = []\n",
    "    \n",
    "        ### Begin your code\n",
    "\n",
    "        ### End your code\n",
    "        \n",
    "        return test_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BestModel()\n",
    "idf = Idf()\n",
    "train_signal_file = \"pa3-data/pa3.signal.train\"\n",
    "train_rel_file = \"pa3-data/pa3.rel.train\"\n",
    "dev_signal_file = \"pa3-data/pa3.signal.dev\"\n",
    "\n",
    "dev_predicts_best = model.train_and_predict(train_signal_file, train_rel_file, dev_signal_file, idf)\n",
    "\n",
    "dev_rel_file = \"pa3-data/pa3.rel.dev\"\n",
    "dev_relevance, dev_ndcg = get_relevance(dev_rel_file)\n",
    "\n",
    "print (\"Average NDCG score:\", NDCG_calc_for_LTR(dev_ndcg, dev_predicts_best, \"ranked_result_best\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code submission\n",
    "\n",
    "You are now ready to submit the code for your assignment. Refer to [submission instructions section](#Submission-instructions). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4: Written Report (20%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section is meant to be relatively more open-ended as you describe the model choices you made in this assignment. Please keep your report concise. Be sure to document any design decisions you made, and provide a brief rationale for them. \n",
    "\n",
    "You may choose to insert cells below to generate tables or plots if required."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. Design of feature vectors (Task 1 and 2) (3%)\n",
    "\n",
    "For each (query, document) pair, in designing your feature vector from query vector and document vectors, you had various possible options for (i) term frequency, (ii) document frequency and (iii) normalization. The default option we recommended you start with for the feature vector is lnn.ltc (using the SMART notation ddd.qqq).\n",
    "\n",
    "What other choices did you experiment with? How did the performance compare across these choices? What might be the rationale for this difference in performance across the various models?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Your Answer Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Hyperparameter tuning  (Task 2) (3%)\n",
    "\n",
    "Briefly describe the hyperparameters you tuned for your implementation of XGBoost. \n",
    "Which hyperparameters were most consequential to the performance of the model?\n",
    "\n",
    "Provide an intuition, based on your understanding of the LambdaMART algorithm, for why the performance of the model varied as it did with the hyperparameters you tuned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Your answer here. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C. Model Design and Ablation Analysis (Task 3) (7%)\n",
    "\n",
    "You had the option to include various additional features in your model design. Which features did you experiment with? Which features did you end up using in your final model, and why? \n",
    "\n",
    "We expect ablation analysis on which features provided useful signals and which ones did not. Please include at least two plots and/or tables for this question."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D. Error Analysis (Task 3) (7%)\n",
    "\n",
    "Analyze your errors for the best performing model you trained. Please include at least two plots and/or tables for this assignment. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Your Answer Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Extra Credit (up to 10%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will give extra credit for best ranking systems in the entire class submitted in Task 3. This is based on the NDCG scores computed on our hidden test data. Include a writeup below that describes the model used, the extensions employed, other models tried, and a hypothesis for why the model used works best. \n",
    "\n",
    "Extra credit will be provided as follows:\n",
    "\n",
    "We will provide:\n",
    "    - 10% for the top few systems in the class (of which 5% is for writeup quality)\n",
    "    - 5% for the next few systems in the class (of which 5% is for writeup quality)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Your writeup here."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 PA3 - Ranking functions (45% of total PA3 grade)\n",
    "\n",
    "In the first part of PA3, you will devise ranking functions to rank results given some queries and corresponding search results. For each query-document pair, you are provided with several features that will help you rank the documents. You are also provided with a training set consisting of query-document pairs along with their relevance values. We will be implementing **three** different ranking functions and will use the **NDCG** metric for evaluating the effectiveness of the ranking function. The estimation of parameters for the ranking functions will be done manually (i.e., no machine learning). \n",
    "\n",
    "More specifically, it involves the following tasks:\n",
    "\n",
    "\n",
    "1. [Cosine Similarity (5%)](#V-Task1:-Cosine-Similarity-(5%)) To implement a variant of cosine similarity (with the L1-Norm) as the ranking function\n",
    "\n",
    "2. [BM25F (15%)](#VI-Task2:-BM25F-(15%)) To implement the BM25F ranking algorithm.\n",
    "\n",
    "3. [Smallest Window (10%)](#VII-Task3:-Smallest-Window-(10%)) Incorporate window sizes into the ranking algorithm from Task 1 (or Task 2 if you prefer). \n",
    "\n",
    "4. [Report (15%)](#Report-(15%)) describing your program and answer a set of questions.\n",
    "\n",
    "\n",
    "__Grading for Tasks 1, 2 and 3__\n",
    "- Half of your grade will be based on your model's performance on an autograder test set. Your scores will be visible to you when you submit on Gradescope, but the test set will not. \n",
    "- The other half of your grade will be based on your model's performance on a hidden test set. Your scores will only be visible to you when grades for this assignment are released\n",
    "- You will get full credit for solutions that receive NDCG scores within reasonable range of the NDCG scores received by the teaching staff.\n",
    "\n",
    "In the next part of PA3 (Learning to rank), you will explore different approaches to learn the parameters for ranking functions using machine learning. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Submission instructions\n",
    "\n",
    "1\\. The assignment is due before class at 4:00 pm on the due date (30th May 2019)\n",
    "\n",
    "2\\. The notebook will automatically generate **python files** in submission folder. You'll have to upload them to the PA3-code assignment on gradescope. Note that you need to upload all the individual files in the submission folder without zipping it.    \n",
    "\n",
    "3\\. While solving the assignment, do **NOT** change class and method names, autograder tests will fail otherwise. \n",
    "\n",
    "4\\. You'll also have to upload a **PDF version** of the notebook (which would be primarily used to grade your report section of the notebook) to PA3-PDF assignment on gradescope. Note that directly converting the PDF truncates code cells. To get a usable PDF version, first click on File > Print Preview, which will open in a new tab, then print to PDF using your browser's print functionality. \n",
    "\n",
    "5\\. Since there are two notebooks, we have included a script to help you merge them together before upload. Run\n",
    "```\n",
    "python pdfcat pa3-ranking.pdf pa3-learning-to-rank.pdf > pa3-solution.pdf\n",
    "``` \n",
    "to generate a single concatenated pdf file and upload `pa3-solution.pdf` to gradescope.\n",
    "\n",
    "6\\. After uploading the PDF make sure you **tag all the relevant pages to each question**. We will penalize for mistagged submissions. \n",
    "\n",
    "7\\. If you are solving the assignment in a team of two, add the other student as a group member after submitting the assignment. Do **NOT** submit the same assignment twice. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the tee magic which saves a copy of the cell when executed\n",
    "%reload_ext autograding_magics\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `submission` folder will contain all the files to be submitted, and `base_classes` contains other class definitions which you will not submit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "try: \n",
    "    os.mkdir('submission')\n",
    "except FileExistsError:\n",
    "    pass\n",
    "try:\n",
    "   open('submission/__init__.py', 'x')\n",
    "except FileExistsError:\n",
    "   pass\n",
    "try: \n",
    "    os.mkdir('base_classes')\n",
    "except FileExistsError:\n",
    "    pass\n",
    "try:\n",
    "   open('base_classes/__init__.py', 'x')\n",
    "except FileExistsError:\n",
    "   pass\n",
    "try: \n",
    "    os.mkdir('output')\n",
    "except FileExistsError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%tee submission/imports.py\n",
    "\n",
    "# You can add additional imports here\n",
    "import sys\n",
    "import pickle as pkl\n",
    "import array\n",
    "import os\n",
    "import timeit\n",
    "import contextlib\n",
    "from collections import OrderedDict, Counter\n",
    "import math\n",
    "\n",
    "import sys\n",
    "from base_classes.load_train_data import load_train_data\n",
    "from base_classes.id_map import IdMap\n",
    "from base_classes.ndcg import NDCG\n",
    "from base_classes.query import Query\n",
    "from base_classes.document import Document\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II. Data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data for this assignment is available as a .zip file at: http://web.stanford.edu/class/cs276/pa/pa3-data.zip. The following code puts the data folder under the current directory. We have partitioned the data into two sets for you: \n",
    "1. Training set of 731 queries (pa3.(signal|rel).train)\n",
    "2. Development set of 124 queries (pa3.(signal|rel).dev)\n",
    "\n",
    "The idea is that while tuning and maximizing performance on the training set, you should also verify how well the tuned parameters are doing on the development set to ensure you are not overfitting your model. There is a hidden test set of 124 queries which we have reserved to grade your final model. For each set, there are two types of files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import zipfile\n",
    "\n",
    "# Download dataset\n",
    "data_dir = 'pa3-data'\n",
    "data_url = 'http://web.stanford.edu/class/cs276/pa/{}.zip'.format(data_dir)\n",
    "urllib.request.urlretrieve(data_url, '{}.zip'.format(data_dir))\n",
    "\n",
    "# Unzip dataset\n",
    "with zipfile.ZipFile('{}.zip'.format(data_dir), 'r') as zip_fh:\n",
    "    zip_fh.extractall()\n",
    "print('Data downloaded and unzipped to {}...\\n'.format(data_dir))\n",
    "\n",
    "# Print the directory structure\n",
    "print('Directory Structure:')\n",
    "print(data_dir + os.path.sep)\n",
    "for sub_dir in os.listdir(data_dir):\n",
    "    if not sub_dir.startswith('.'):\n",
    "        print('  - ' + sub_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Signal File \n",
    "**– pa3.signal.(train|dev):** lists queries along with documents returned by a widely used search engine for each individual query (the list of documents is shuffled and is not in the same order as returned by the search engine). Each query has 10 or less documents. For example, the format for a pair of query/document (qd) is as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = os.path.join(data_dir, \"pa3.signal.train\")\n",
    "with open(filename, 'r', encoding = 'utf8') as f:\n",
    "    print(f.read()[0:1000])\n",
    "print(\"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This pattern repeats for the next url until all of the urls for this query are done and then the overall pattern repeats for the next query. There is only one <b>title</b>, <b>pagerank</b>, and <b>body length</b> for each url but there can be multiple <b>header</b>, <b>body hits</b> and <b>anchor text</b> (and corresponding stanford anchor count) lines.\n",
    "\n",
    "* The <b>body hits</b> line specifies the term followed by the positional postings list of that term in the document (sorted in increasing order).\n",
    "* The <b>body length</b> line states how many terms are present in the body of the document.\n",
    "* The <b>stanford anchor count</b>, specified immediately after the anchor text line, states how many anchors there are on the stanford.edu domain with that anchor text. For example, if the anchor text is “stanford math department” and the count is 9, that means there are nine links to the current page (from other pages) where the anchor text is “stanford math department”.\n",
    "* The <b>pagerank</b> is an integer from 0 to 9 that signifies a query-independent quality of the page (the higher the PageRank, the better the quality of the page)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relevance File\n",
    "**– pa3.rel.(train|dev)**: lists the relevance judgments for each of the query-document pairs in the corresponding signal file. The collected relevance data was an integer ranging from −1 to 3 with a higher value indicating that the document is more relevant to that query. We have averaged relevance scores for each query-url pair with −1 ignored. For example, the format of this document is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = os.path.join(data_dir, \"pa3.rel.train\")\n",
    "with open(filename, 'r', encoding = 'utf8') as f:\n",
    "    print(f.read()[0:199])\n",
    "print(\"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This pattern repeats for the next query until all of the queries in the file are done. The url line can be broken into the document url and the relevance judgment for the query-document pair."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ranking functions also require certain collection-wide statistics (such as inverse document frequency) and we cannot infer this information just from the training set itself. We provide **docs.dict, terms.dict and BSBI.dict** what you generated from PA1, and leave you to calculate idf below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# III. Normalized Discounted Cumulative Gain (NDCG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The evaluation metric used is Normalized Discounted Cumulative Gain (NDCG) since we are using a non-binary relevance metric. Since each query has at most 10 results returned, we use NDCG for the first 10 search results.\n",
    "Then, for a particular query q,\n",
    "$$NDCG(q) = \\frac{1}{Z} \\sum_{m=1}^{p}\\frac{2^{R(q,m)}-1}{log_{2}(1+m)}$$\n",
    "Here, $R(q, m)$ is the relevance judgment given to document $m$ for query $q$. $Z$ is a normalization factor. It is the ideal NDCG value. The ideal NDCG (iNDCG) value is calculated by ordering the documents in decreasing order of relevance and calculating the NDCG value with $Z=1$. If iNDCG is zero, $NDCG(q) = 1$. Finally, $p$ is the number of documents that are possible matches for that query.\n",
    "\n",
    "We can compute the NDCG for a set of queries $Q = \\{q_1,...,q_m\\}$ by taking the average of the NDCGs for each of the individual queries. \n",
    "\n",
    "The starter code [Section NDCG](#VII.1-NDCG-implementation) contains a Python implementation of NDCG which you can use directly to evaluate your ranking function on the training data. We will be using the same method to evaluate your ranking on grading data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IV. Ranking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IV.1 Term Score\n",
    "In the signal files of the training data, each query-document pair provides term information from five different fields: <b>url</b>, <b>title</b>, <b>headers</b>, <b>body</b> and <b>anchors</b>. Additionally each pair provides <b>pagerank</b> but we won’t be using it in cosine similarity. Even for BM25F, we will consider it separately as explained in [BM25F](#VI-Task2:-BM25F-(15%)). Each of the required ranking functions will construct a term score ($tf$) vector for each query-document pair from hits in these different fields. All of our ranking functions only care about terms that occur in the query.\n",
    "\n",
    "The raw term score vector, $rs$, counts how many times a query term occurs in a field. For the <b>anchor</b> field, we assume that there is one big document that contains all of the anchors with the anchor text multiplied by the anchor count. A similar approach can be followed for the <b>header</b> field as well. Thus, in the $qd$ example whose term-vector is $[\\text{stanford aoerc pool hours}]^T$ (which is shown in above printing of signal file), the $rs$\n",
    "vector for the body field will be $[{10 \\ 7 \\ 1 \\ 0}]^T$ as there are 10 hits for the term “stanford” in the <b>body</b> field and 7 hits for the term “aoerc” as well as 1 hits for the term “pool”. Similarly, the $rs$ vector for <b>anchor</b> field will be $[\\text{0 0 0 0}]^T$ as there is no anchor for this document. For <b>anothe</b>r example \n",
    "```python\n",
    "  url: https://cardinalrec.stanford.edu/facilities/aoerc/\n",
    "    ...\n",
    "    anchor_text: gyms aoerc\n",
    "      stanford_anchor_count: 3\n",
    "    anchor_text: aoerc\n",
    "      stanford_anchor_count: 13\n",
    "    anchor_text: http cardinalrec stanford edu facilities aoerc\n",
    "      stanford_anchor_count: 4\n",
    "    anchor_text: arrillaga outdoor education and recreation center aoerc link is external\n",
    "      stanford_anchor_count: 1\n",
    "    anchor_text: the arrillaga outdoor education and research center aoerc\n",
    "      stanford_anchor_count: 2\n",
    "    anchor_text: aoerc will shutdown for maintenance\n",
    "      stanford_anchor_count: 2\n",
    "```\n",
    "\n",
    "The <b>anchor</b> will be $[\\text{4 25 0 0}]^T$ as there is 4 stanford_anchor_count for term “stanford” and 25 stanford_anchor_count for term “aoerc”.\n",
    "\n",
    "Finally, the $rs$ vector <br>\n",
    "for the <b>title</b> field is $[\\text{1 0 0 0}]^T$,<br>\n",
    "for the <b>url</b> field is$[\\text{1 0 0 0}]^T$, <br>\n",
    "for the <b>header</b> field is $[\\text{5 0 0 1}]^T$ . \n",
    "\n",
    "Note that in order to extract <b>url</b> hits, you will have to tokenize the url on non-alphanumeric characters. We've provided the parser code for you.\n",
    "\n",
    "While calculating the raw term scores, we convert everything to lowercase and then calculate the counts. The <b>body_hits</b> field given in the data do not perform any stemming. However, for the other fields, you are free to experiment with different techniques like stemming etc. You may find [nltk](https://www.nltk.org/) could be useful "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IV.2 Output Requirements\n",
    "In all three tasks, the goal is to derive specific types of ranking functions based on the training data and relevance values. Once the ranking function $rf$ has been crafted, we will then pass in the test data set and your application must use $rf$ to rank the query-document pairs and output the list of documents for each query in decreasing rank order. The NDCG evaluation metric will then be applied on these lists against the evaluation provided by you in the search ratings task earlier in the course. The higher the value, the better your ranking algorithm works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We predefine Query and Document class for you. You can load training data and construct a query dictionary by load_train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = os.path.join(data_dir, \"pa3.signal.train\")\n",
    "query_dict = load_train_data(file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# Mapping of Query-url-Document. Query -> (url -> Document)\n",
    "query_dict[Query(\"stanford aoerc pool hours\")]  # Access a query \n",
    "query_dict[Query(\"stanford aoerc pool hours\")]['an url']  # Access a document \n",
    "query_dict[Query(\"stanford aoerc pool hours\")]['an url'].body_hits  # Access a field of document \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_doc = query_dict[Query(\"stanford aoerc pool hours\")]['http://events.stanford.edu/2014/February/18/']\n",
    "print(\"document:\", sample_doc)\n",
    "print(\"url\", sample_doc.url)\n",
    "print(\"headers:\", sample_doc.headers)\n",
    "print(\"body_hits:\",sample_doc.body_hits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IV.4 Build Idf Dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, you will need to build an idf dictionary contain idf of a term, which will be used later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%tee submission/build_idf.py\n",
    "import pickle as pkl\n",
    "import math\n",
    "class Idf:\n",
    "    \"\"\"Build idf dictionary and return idf of a term, whether in or not in built dictionary.\n",
    "        Recall from PA1 that postings_dict maps termID to a 3 tuple of \n",
    "        (start_position_in_index_file, number_of_postings_in_list, length_in_bytes_of_postings_list)\n",
    "        \n",
    "        Remember that it's possible for a term to not appear in the collection corpus.\n",
    "        Thus to guard against such a case, we will apply Laplace add-one smoothing.\n",
    "        \n",
    "        Note: We expect you to store the idf as {term: idf} and handle term which is not in posting_list\n",
    "\n",
    "        Hint: For term not in built dictionary, we should return math.log10(total_doc_num / 1.0).\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"Build an idf dictionary\"\"\"\n",
    "        try:\n",
    "            # We provide docs.dict, terms.dict and BSBI.dict what you generated from PA1\n",
    "            with open(\"pa3-data/docs.dict\", 'rb') as f:\n",
    "                docs = pkl.load(f)\n",
    "            self.total_doc_num = len(docs)\n",
    "            print(\"Total Number of Docs is\", self.total_doc_num)\n",
    "\n",
    "            with open(\"pa3-data/terms.dict\", 'rb') as f:\n",
    "                terms = pkl.load(f)\n",
    "            self.total_term_num = len(terms)\n",
    "            print(\"Total Number of Terms is\", self.total_term_num)\n",
    "\n",
    "            with open('pa3-data/BSBI.dict', 'rb') as f:\n",
    "                postings_dict, termsID = pkl.load(f)\n",
    "\n",
    "            self.idf = {}\n",
    "            ### Begin your code\n",
    "\n",
    "            ### End your code\n",
    "        except FileNotFoundError:\n",
    "            print(\"doc_dict_file / term_dict_file Not Found!\")\n",
    "\n",
    "    def get_idf(self, term = None):\n",
    "        \"\"\"Return idf of return idf of a term, whether in or not in built dictionary.\n",
    "        Args:\n",
    "            term(str) : term to return its idf\n",
    "        Return(float): \n",
    "            idf of the term\n",
    "        \"\"\"\n",
    "        ### Begin your code\n",
    "\n",
    "        ### End your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_idf = Idf()\n",
    "my_idf.get_idf(\"data\")\n",
    "assert len(my_idf.idf) == 347071, 'Not matching with expected length of idf.' \n",
    "assert my_idf.get_idf(\"bilibalabulu\") > 4.9, \"Not handle unseen term or give wrong value\"\n",
    "assert my_idf.get_idf(\"data\") < my_idf.get_idf(\"radiology\"), 'idf of rarer terms should be larger than common terms.'\n",
    "assert my_idf.get_idf(\"to\") < my_idf.get_idf(\"design\"), 'idf of rarer terms should be larger than common terms.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# V Task1: Cosine Similarity (5%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first task is to implement a variant of cosine similarity (with the L1-Norm) as the ranking function. This essentially involves constructing the <b><i>document vector</i></b> and the <b><i>query vector</i></b> and then taking their dot product. Recall from Figure 6.15 in the textbook that in order to construct the vectors, we need to decide on how we compute a term frequency, a document frequency weighting, and a normalization strategy. Let’s discuss these for both the vectors separately.\n",
    "<img src=\"fig/IIR_fig_6.15.png\">\n",
    "Figure is from Pg.128 http://nlp.stanford.edu/IR-book/pdf/06vect.pdf\n",
    "\n",
    "Note: We will only grade Task 1 on default parameter to check the correctness of your implementation. But it could be helpful to do parameter tuning on it and have a sense of the importance of each field. You will need that in Task 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V.1 Query vector \n",
    "\n",
    "* Term frequency<br>\n",
    "The raw term frequencies can be computed using the query (should be 1 for most queries but not necessarily true). Again, you can use either the raw frequencies or sublinearly scale them.\n",
    "\n",
    "\n",
    "* Document frequency<br>\n",
    "Each of the terms in <i>qv</i> should be weighted using the idf value for each of the terms in the query. Computing the idf above from the corpus from PA1 to determine how many documents contain the query terms. One issue is that it is possible for a query term <i>t</i> to not appear in the collection corpus and it is not possible to evaluate ${\\text{idf}_t}$. In such a case, we will apply the Laplace add-one smoothing technique learned earlier in the course 3. (This essentially assumes the existence of a hypothetical dummy document that contains all possible terms, and therefore, adds 1 to each numerator and denominator with the idft formula.)\n",
    "\n",
    "\n",
    "* Normalization<br>\n",
    "No normalization is needed for query length because any query length normalization applies to all docs and so is not relevant to ranking.\n",
    "\n",
    "**Note**: We ask you to implement the b-t-n (boolean-idf-none) scheme for query vector and check the correctness of your scorer based on this default setting. You could select other reasonable scheme to increase the performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V.2 Document vector\n",
    "* Term frequency<br>\n",
    "We compute the raw term frequencies for each query term in the different fields using the method described in [Section IV.1](#IV.1-Term-Score) . For each of the fields, we can compute the <i>tf</i> vector, either using the raw scores themselves or by applying sublinear scaling on the raw scores. In sublinear scaling, we have $tf_i = 1 + log(rs_i)$ if $rs_i > 0$ and $0$ otherwise. Thus, the <i>tf</i> vector for the <b>body</b> field for qd will be $[\\text{1+log(10)  1+log(7)  1+log(1)  0}]^T$ . \n",
    "More information about sublinear tf scaling is described in <a href=\"http://nlp.stanford.edu/IR-book/pdf/06vect.pdf\"> Page 126 Section 6.4.1 of the textbook</a>.\n",
    "\n",
    "\n",
    "* Document frequency<br>\n",
    "We will not use any document frequency in the document vector. Instead, it is incorporated in the query vector as described below.\n",
    "\n",
    "\n",
    "* Normalization<br>\n",
    "We cannot use cosine normalization as we do not have access to the contents of the document and, thus, do not know what other terms (and counts of those terms) occur in the <b>body</b> field. As a result, we use length normalization instead. Moreover, since there can be huge discrepancies between the lengths of the different fields, we divide all fields by the same normalization factor, the body length. <br> Note that some documents have a body length of 0, so you will have to smooth them somehow. A good strategy is to add a value, say 500, to the body length of each document. You can experiment with this value or with other smoothing strategies and report them.\n",
    "\n",
    "Note: We ask you to implement the n-n-n* (natural-no- some normalization*) scheme for document vector for and check the correctness of your scorer based on this default setting. You could select other reasonable scheme to increase the performance.\n",
    "\n",
    "**Hint:** The normalizaton of document vector of task 1 and task 2 are different but the tasks could share same term frequency and document frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that to fully test the correctness of your scorer, we provide the defaut weight scheme for query vector and doc vector. You should **implement the defaut ones and any other variants that you believe will increase the performance**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V.3 Abstract Scorer and Baseline Scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use a sample q and d to help/assert your score implementation \n",
    "q = Query(\"stanford aoerc pool hours\")\n",
    "d = query_dict[q]['http://events.stanford.edu/2014/February/18/'] # example that has body_hits\n",
    "# d = query_dict[q]['https://cardinalrec.stanford.edu/facilities/aoerc/']  # example that has anchors\n",
    "print(\"Query q: \", q)\n",
    "print(\"Document d: \", d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%tee submission/ascore.py\n",
    "import math\n",
    "from collections import Counter\n",
    "class AScorer:\n",
    "    \"\"\" An abstract class for a scorer. \n",
    "        Implement query vector and doc vector.\n",
    "        Needs to be extended by each specific implementation of scorers.\n",
    "    \"\"\"\n",
    "    def __init__(self, idf, query_weight_scheme=None, doc_weight_scheme=None): #Modified\n",
    "        self.idf = idf\n",
    "        self.TFTYPES = [\"url\",\"title\",\"body_hits\",\"header\",\"anchor\"]\n",
    "        \n",
    "        self.default_query_weight_scheme = {\"tf\": 'b', \"df\": 't', \"norm\": None} # boolean, idf, none\n",
    "        self.default_doc_weight_scheme = {\"tf\": 'n', \"df\": 'n', \"norm\": None}   # natural, none\n",
    "        \n",
    "        self.query_weight_scheme = query_weight_scheme if query_weight_scheme is not None \\\n",
    "                                   else self.default_query_weight_scheme #Modified (added)\n",
    "        self.doc_weight_scheme = doc_weight_scheme if doc_weight_scheme is not None \\\n",
    "                                 else self.default_doc_weight_scheme #Modified (added)\n",
    "\n",
    "    def get_sim_score(self, q, d):\n",
    "        \"\"\" Score each document for each query.\n",
    "        Args:\n",
    "            q (Query): the Query\n",
    "            d (Document) :the Document\n",
    "\n",
    "        Returns:\n",
    "            pass now, will be implement in task 1, 2 and 3\n",
    "        \"\"\"        \n",
    "        raise NotImplementedError\n",
    "\n",
    "    # Include any initialization and/or parsing methods that \n",
    "    # you may want to perform on the Document fields prior to accumulating counts.\n",
    "    # See the Document class to see how the various fields are represented\n",
    "    # We have provided a few parser functions for you. Feel free to change them, and add more if you find its useful\n",
    "\n",
    "    ### Begin your code\n",
    "\n",
    "    ### End your code\n",
    "    def parse_url(self, url, token=False):\n",
    "        \"\"\"Parse document's url. Return Counter of url's tokens\"\"\"\n",
    "        # token indicate whether we want the raw token or Counter of it\n",
    "        if url:\n",
    "            url_token_in_term = url.replace(\"http:\",\".\").replace('/','.').replace('?','.') \\\n",
    "                                   .replace('=','.').replace(\"%20\",\".\").replace(\"...\",\".\").replace(\"..\",\".\")\\\n",
    "                                   .lower();\n",
    "            url_token = url_token_in_term.split('.')[1:]\n",
    "            if token:\n",
    "                return url_token \n",
    "            else:\n",
    "                return Counter(url_token)\n",
    "        return Counter([])\n",
    "\n",
    "    def parse_title(self, title, token=False):\n",
    "        \"\"\"Parse document's title. Return Counter of title's tokens\"\"\"\n",
    "        if title:\n",
    "            if token:\n",
    "                return title.split(\" \") \n",
    "            else:\n",
    "                return Counter(title.split(\" \"))\n",
    "        else:\n",
    "            return Counter([])\n",
    "\n",
    "    def parse_headers(self, headers):\n",
    "        \"\"\"Parse document's headers. Return Counter of headers' tokens\"\"\"\n",
    "        headers_token = []\n",
    "        if headers is not None:\n",
    "            for header in headers:\n",
    "                header_token = header.split(\" \")\n",
    "                headers_token.extend(header_token)\n",
    "        return Counter(headers_token)\n",
    "\n",
    "    def parse_anchors(self, anchors):\n",
    "        \"\"\"Parse document's anchors. Return Counter of anchors' tokens\"\"\"\n",
    "        anchor_count_map = Counter({})\n",
    "        if anchors is not None:\n",
    "            for anchor in anchors:\n",
    "                count = anchors[anchor]\n",
    "                anchor_tokens = anchor.split(\" \")\n",
    "                for anchor_token in anchor_tokens:\n",
    "                    if(anchor_token in anchor_count_map.keys()):\n",
    "                        anchor_count_map[anchor_token] += count\n",
    "                    else:\n",
    "                        anchor_count_map[anchor_token] = count           \n",
    "        return anchor_count_map\n",
    " \n",
    "    def parse_body_hits(self, body_hits):\n",
    "        \"\"\"Parse document's body_hits. Return Counter of body_hits' tokens\"\"\"\n",
    "        body_hits_count_map = Counter({})\n",
    "        if body_hits is not None:\n",
    "            for body_hit in body_hits:\n",
    "                body_hits_count_map[body_hit] = len(body_hits[body_hit])\n",
    "        return body_hits_count_map\n",
    "    \n",
    "    \n",
    "    def get_query_vector(self, q, query_weight_scheme=None):\n",
    "\n",
    "        \"\"\" Handle the query vector. \n",
    "        1. get term freq 2. get doc freq 3. normalization\n",
    "        Refer to above SMART notificaton and figure\n",
    "        \n",
    "        Compute the raw term (and/or sublinearly scaled) frequencies\n",
    "        Additionally weight each of the terms using the idf value of the term in the query \n",
    "        (we use the PA1 corpus to determine how many documents contain the query terms \n",
    "        which is calculated above and stored in self.idf).\n",
    "        \n",
    "        Note that no normalization is needed for query length \n",
    "        because any query length normalization applies to all docs and so is not relevant to ranking.\n",
    "        \n",
    "        Args:\n",
    "            q (Query): Query(\"some query\")\n",
    "            \n",
    "        Returns:\n",
    "            query_vec (dict):  the query vector\n",
    "        \"\"\"  \n",
    "       \n",
    "        if query_weight_scheme is None:\n",
    "            query_weight_scheme = self.query_weight_scheme #modified\n",
    "            \n",
    "        query_vec = {}\n",
    "        ### Begin your code\n",
    "\n",
    "        ### End your code\n",
    "        return query_vec\n",
    "    \n",
    "    def get_doc_vector(self, q, d, doc_weight_scheme=None):\n",
    "        \n",
    "        \"\"\"get term freqs for documents\n",
    "        You will need to \n",
    "        1. Initialize tfs for tf types (as in self.TFTYPES)\n",
    "        2. Initialize tfs for query_words\n",
    "        3. Tokenize url, title, and headers, anchors, body_hits if exits\n",
    "        4. (we've already provided parse functions above)\n",
    "        5. Loop through query terms increasing relevant tfs\n",
    "        \n",
    "        Args:\n",
    "        q (Query) : Query(\"some query\")\n",
    "        d (Document) : Query(\"some query\")[\"an url\"]\n",
    "        \n",
    "        Returns:\n",
    "        doc_vec (dict) :A dictionary of doc term frequency:\n",
    "                    tf type -> query_word -> score\n",
    "                    For example: the output of document d\n",
    "                    Should be look like \"{'url': {'stanford': 1, 'aoerc': 0, 'pool': 0, 'hours': 0},\n",
    "                                     'title': {'stanford': 1, 'aoerc': 0, 'pool': 0, 'hours': 0},...\"\"\n",
    "        \"\"\"\n",
    "        if doc_weight_scheme is None:\n",
    "            doc_weight_scheme = self.doc_weight_scheme #modified\n",
    "            \n",
    "        doc_vec = {} \n",
    "        \n",
    "        ### Begin your code\n",
    "\n",
    "        ### End your code\n",
    "        \n",
    "        # Normalization\n",
    "        if doc_weight_scheme['norm']:\n",
    "            norm_func = doc_weight_scheme[\"norm\"]\n",
    "            doc_vec = norm_func(q, d, doc_vec)\n",
    "        return doc_vec\n",
    "        \n",
    "        \n",
    "    def normalize_doc_vec(self, q, d, doc_vec):\n",
    "        \"\"\" Normalize the doc vector\n",
    "        Task 1 and 2 will use different normlization. You can also try other different normalization methods.\n",
    "        Args: \n",
    "            doc_vec (dict) : the doc vector\n",
    "            q (Query) : the query\n",
    "            d (Document) : the document\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    # For the learning-to-rank ipython notebook, you may choose to define additional function(s)\n",
    "    # below for various possible kinds of normalization. \n",
    "    # You will not need to fill this section out for the \"ranking\" notebook. \n",
    "\n",
    "    ### Begin your code\n",
    "\n",
    "    ### End your code \n",
    "    \n",
    "    def get_net_score(self, q, query_vec, d, doc_vec):\n",
    "        \"\"\" calculate net score\n",
    "        Args:\n",
    "            q (Query) : the query\n",
    "            query_vec (dict) : the query vector\n",
    "            d (Document) : the document\n",
    "            doc_vec (dict) : the document vector\n",
    "        Return:\n",
    "            score (float) : the net score\n",
    "        \"\"\"\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Free free to compare your get_doc_vector result with above instruction and your own understanding. \n",
    "We did not use  other techniques such as stemming. But free to do that yourself to increase the performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_scorer = AScorer(my_idf)\n",
    "query_vec = a_scorer.get_query_vector(q, None) \n",
    "print(query_vec)\n",
    "doc_vec = a_scorer.get_doc_vector(q, d, None) \n",
    "doc_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IV.4 Baseline Score\n",
    "\n",
    "Here we provide the a baseline score to partially test your implementation of get_query_vector and get_doc_vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%tee base_classes/baseline_score.py\n",
    "class BaselineScorer(AScorer):\n",
    "    def __init__(self, idf):\n",
    "        super().__init__(idf)\n",
    "    \n",
    "    def get_sim_score(self, q, d):\n",
    "        q_vec = self.get_query_vector(q)\n",
    "        d_vec = self.get_doc_vector(q, d)\n",
    "        score = 0\n",
    "        if 'body_hits' in d_vec.keys():\n",
    "            for term in d_vec['body_hits'].keys():\n",
    "                score += d_vec['body_hits'][term]\n",
    "        return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_scorer = BaselineScorer(my_idf)\n",
    "print('query vector: ',  baseline_scorer.get_query_vector(q))\n",
    "print('doc vector', baseline_scorer.get_doc_vector(q, d))\n",
    "assert baseline_scorer.get_sim_score(q, d) == 18, \"Similarity scorer using default weight scheme for q and d \\\n",
    "                                                   does not match with our results\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a document $d$ and query $q$, if $qv_q$ is the query vector and $tf_{d,u}$, $tf_{d,t}$, $tf_{d,b}$, $tf_{d,h}$ and $tf_{d,a}$ are the term score vector for the <b>url</b>, <b>title</b>, <b>body</b>, <b>header</b> and <b>anchor fields</b>, respectively, then the net score is\n",
    "$$qv_q \\cdot (c_u \\cdot tf_{d,u} + c_t \\cdot tf_{d,t} + c_b \\cdot tf_{d,b} + c_h \\cdot tf_{d,h} + c_a \\cdot tf_{d,a})$$\n",
    "\n",
    "Here, $c_u$, $c_t$, $c_b$, $c_h$ and $c_a$ are the weights given to <b>url</b>, <b>title</b>, <b>body</b>, <b>header</b> and <b>anchor fields</b>, respectively.\n",
    "\n",
    "The goal is to determine the weights for all 5 fields (and, thus, the ranking function using cosine similarity) so that the NDCG function is of an optimal value when run on the test set. You will use the training set given to derive the above parameters.\n",
    "\n",
    "**Hint**: Note that the absolute values of weights won’t matter as they will be the same for all documents, only the relative weights for different fields is important; i.e. you can multiply each weight by a constant and the ranking will remain the same. In order to estimate the relative weights, try to reason the relative importance of the different fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_params_cosine = {\n",
    "    \"url_weight\" : 10,\n",
    "    \"title_weight\": 0.1,\n",
    "    \"body_hits_weight\" : 0.1,\n",
    "    \"header_weight\" : 0.1,\n",
    "    \"anchor_weight\" : 0.1,\n",
    "    \"smoothing_body_length\" : 800,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%tee submission/cosine_score.py\n",
    "from submission.ascore import * #modified\n",
    "class CosineSimilarityScorer(AScorer):\n",
    "\n",
    "    def __init__(self, idf, query_dict, params, query_weight_scheme=None, doc_weight_scheme=None): #Modified\n",
    "        # query_dict is unnecessary for CosineSimilarityScorer,\n",
    "        # but it's useful for child class SmallestWindowScorer\n",
    "        super().__init__(idf, query_weight_scheme=query_weight_scheme, doc_weight_scheme=doc_weight_scheme) #Modified\n",
    "        self.url_weight = params[\"url_weight\"]\n",
    "        self.title_weight  = params[\"title_weight\"]\n",
    "        self.body_hits_weight = params[\"body_hits_weight\"]\n",
    "        self.header_weight = params[\"header_weight\"]\n",
    "        self.anchor_weight = params[\"anchor_weight\"]\n",
    "        self.smoothing_body_length = params[\"smoothing_body_length\"]\n",
    "        \n",
    "    def get_net_score(self, q, query_vec, d, doc_vec):\n",
    "        \"\"\" calculate net score\n",
    "        Args:\n",
    "            q (Query) : the query\n",
    "            query_vec (dict) : the query vector\n",
    "            d (Document) : the document\n",
    "            doc_vec (dict) : the document vector\n",
    "        Return:\n",
    "            score (float) : the net score\n",
    "        \"\"\"\n",
    "        ### Begin your code\n",
    "\n",
    "        ### End your code\n",
    "        return score\n",
    "    \n",
    "    \n",
    "    ## Normalization\n",
    "    def L1_normalize_doc_vec(self, q, d, doc_vec): \n",
    "        \"\"\" Normalize the doc vector\n",
    "        Note that we should give uniform normalization to all fields\n",
    "        as discussed in Session V.2 Document vector - Normalization.\n",
    "        Args: \n",
    "            q (Query) : the query\n",
    "            d (Document) : the document\n",
    "            doc_vec (dict) : the doc vector\n",
    "        Return:\n",
    "            doc_vec (dict) : the doc vector after normalization\n",
    "        \"\"\"\n",
    "        ### Begin your code\n",
    "\n",
    "        ### End your code    \n",
    "        \n",
    "        \n",
    "    def get_sim_score(self, q, d):\n",
    "        \"\"\" Get the similarity score between a document and a query.\n",
    "        Args:\n",
    "            q (Query) : the query\n",
    "            d (Document) : the document\n",
    "            \n",
    "        Return: the similarity score of q and d\n",
    "        \"\"\"\n",
    "        query_vec = self.get_query_vector(q) \n",
    "        # Define normalizattion functon here or directly pass in normalize_func as shown in below cell\n",
    "        self.doc_weight_scheme['norm'] = self.L1_normalize_doc_vec #modified\n",
    "        # Normalization\n",
    "        norm_doc_vec = self.get_doc_vector(q, d, self.doc_weight_scheme) #modified\n",
    "        return self.get_net_score(q, query_vec, d, norm_doc_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cs = CosineSimilarityScorer(my_idf, query_dict, default_params_cosine)\n",
    "\n",
    "query_weight_scheme = {\"tf\": 'b', \"df\": 't', \"norm\": None} \n",
    "doc_weight_scheme = {\"tf\": 'n', \"df\": 'n', \"norm\": cs.L1_normalize_doc_vec}\n",
    "\n",
    "print('QUERY Vector: ',  cs.get_query_vector(q, query_weight_scheme), '\\n')\n",
    "\n",
    "print('unnormalized doc vector', cs.get_doc_vector(q, d, None), '\\n')\n",
    "print('score after normalize doc vector', cs.get_sim_score(q, d), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VI Task2: BM25F (15%) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second task is to implement the BM25F ranking algorithm. The algorithm is decribed in detail in the lecture slides. Specifically, you should have a look at BM25F related slides of [04/25 lecture](http://web.stanford.edu/class/cs276/19handouts/lecture7-probir-1per.pdf) before reading further. Here, instead of using the term scores from [Section IV.1](##IV.1-Term-Score), we use field-dependent normalized term frequency ($ftf$). Thus, for a given term $t$ and field $f \\in \\{url, header, body, title, anchor\\}$ in document $d$, \n",
    "\n",
    "\\begin{equation} \n",
    "ftf_{d,f,t} = \\frac{tf_{d,f,t}}{1 + B_f((\\text{len}_{d,f} / \\text{avlen}_f) - 1)} \n",
    "\\tag{1}\n",
    "\\end{equation}\n",
    "where $tf_{d,f,t}$ is the raw term frequency of $t$ in field $f$ in document $d$, $len_{d,f}$ is the length of $f$ in $d$ and $avlen_f$ is the average field length for $f$. The variables $avlen_{body}$, $avlen_{url}$, $avlen_{title}$, $avlen_{header}$ and $avlen_{anchor}$ can be computed using the training set. $B_f$ is a field-dependent parameter and must be tuned for this task. If $avlen_f$ is zero (should not happen in this dataset), then $ftf_{d,f,t} = 0$.\n",
    "\n",
    "Then, the overall weight for the term $t$ in document $d$ among all fields is \n",
    "\\begin{equation} \\label{wtd}\n",
    "w_{d,t} = \\sum_{f}W_f \\cdot ftf_{d,f,t}\n",
    "\\tag{2}\n",
    "\\end{equation}\n",
    "Here, $W_f$ is also a field-dependent parameter that determines the relative weights given to each field. This value is similar in theory to the tuning parameters for Task 1. \n",
    "\n",
    "\n",
    "Since, we also have a non-textual feature, in the form of <b>pagerank</b>, we incorporate it into our ranking function using the method described in the BM25 lecture regarding ranking with non-textual features.\n",
    "\n",
    "\n",
    "Therefore, the overall score of document $d$ for query $q$ is then:\n",
    "\\begin{equation} \\label{bmeqn}\n",
    "\\sum_{t \\in q} \\frac{w_{d,t}}{K_1 + w_{d,t}}idf_t + \\lambda V_{j}(f)\n",
    "\\tag{3}\n",
    "\\end{equation}\n",
    "where $K_1$ is also a free parameter and $V_{j}$ can be a log/saturation/sigmoid function as mentioned in the slides (you will need to experiment with the other parameter $\\lambda^\\prime$ used by the $V_{j}$ function).\n",
    "\n",
    "Thus, for this task, there are a minimum of 13 parameters to optimize, namely $B_{url}, B_{title}, B_{header}$, $B_{body}, B_{anchor}$, $W_{url}, W_{title}, W_{header}$, $W_{body}, W_{anchor}$, $\\lambda, \\lambda^\\prime$ and $K_1$. Additionaly, you also have to select the $V_{j}$ function appropriately.\n",
    "\n",
    "While in theory, BM25F should give a better NDCG value as it incorporates a lot of more information, this need not necessarily be the case. \n",
    "\n",
    "**Hint**: The weight values obtained in Task1 may be a good starting point for this task. Again note that the weights will depend on the \"importance\" of the fields. Moreover, as mentioned in the slides, log(pagerank) works well in practice but you should try other functions as well and see how they work.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_params_bm25f = {\n",
    "    \"url_weight\" : 0.1,\n",
    "    \"title_weight\": 0.1,\n",
    "    \"body_hits_weight\" : 0.1,\n",
    "    \"header_weight\" : 0.1,\n",
    "    \"anchor_weight\" : 0.1,\n",
    "    \"b_url\" : 0.1,\n",
    "    \"b_title\" : 0.1,\n",
    "    \"b_header\" : 0.1,\n",
    "    \"b_body_hits\" : 0.1,\n",
    "    \"b_anchor\" : 0.1,\n",
    "    \"k1\": 0.1,\n",
    "    \"pagerank_lambda\" : 0.1,\n",
    "    \"pagerank_lambda_prime\" : 0.1, \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%tee submission/params_bm25f.py\n",
    "### Begin your code\n",
    "params_bm25f = {\n",
    "    \n",
    "}\n",
    "### End your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%tee submission/bm25f_score.py\n",
    "from submission.ascore import * #modified\n",
    "class BM25FScorer(AScorer):\n",
    "\n",
    "    def __init__(self, idf, query_dict, params, query_weight_scheme=None, doc_weight_scheme=None): #modified\n",
    "        super().__init__(idf, query_weight_scheme=query_weight_scheme, doc_weight_scheme=doc_weight_scheme) #modified\n",
    "        self.query_dict = query_dict\n",
    "        \n",
    "        self.url_weight = params['url_weight']\n",
    "        self.title_weight  = params['title_weight']\n",
    "        self.body_hits_weight = params['body_hits_weight']\n",
    "        self.header_weight = params['header_weight']\n",
    "        self.anchor_weight = params['anchor_weight']\n",
    "        # bm25 specific weights\n",
    "        self.b_url = params['b_url']\n",
    "        self.b_title = params['b_title']\n",
    "        self.b_header = params['b_header']\n",
    "        self.b_body_hits = params['b_body_hits']\n",
    "        self.b_anchor = params['b_anchor']\n",
    "        self.k1 = params['k1']\n",
    "        self.pagerank_lambda = params['pagerank_lambda']\n",
    "        self.pagerank_lambda_prime = params['pagerank_lambda_prime']\n",
    "\n",
    "        # BM25F data structures feel free to modify these\n",
    "        # Document -> field -> length\n",
    "        self.length = {}\n",
    "        self.avg_length = {}\n",
    "        self.pagerank_scores = {}\n",
    "        \n",
    "        self.calc_avg_length()\n",
    "        \n",
    "    def calc_avg_length(self):\n",
    "        \"\"\" Set up average lengths for BM25F, also handling PageRank. \n",
    "        You need to \n",
    "        Initialize any data structures needed.\n",
    "        Perform any preprocessing you would like to do on the fields.\n",
    "        Handle pagerank\n",
    "        Accumulate lengths of fields in documents. \n",
    "        Hint: You could use query_dict\n",
    "        \"\"\"\n",
    "        ### Begin your code\n",
    "\n",
    "        ### End your code\n",
    "        \n",
    "    def get_net_score(self, q, query_vec, d, doc_vec):\n",
    "        \"\"\" Compute the overall score using above equation\n",
    "        Args:\n",
    "            q (Query) : the query\n",
    "            query_vec (dict) : the query vector\n",
    "            d (Document) : the document\n",
    "            doc_vec (dict) : the doc vector\n",
    "        Return:\n",
    "            score (float) : the net score\n",
    "        \"\"\"\n",
    "        ### Begin your code\n",
    "\n",
    "        ### End your code\n",
    "        return score\n",
    "    \n",
    "    \n",
    "    def bm25f_normalize_doc_vec(self, q, d, doc_vec):\n",
    "        \"\"\" Normalize the raw term frequencies in fields in document d \n",
    "            using above equation (1).\n",
    "        Args:\n",
    "            q (Query) : the query       \n",
    "            d (Document) : the document\n",
    "            doc_vec (dict) : the doc vector\n",
    "        Return: \n",
    "            doc_vec (dict) : the doc vector after normalization\n",
    "        \"\"\"\n",
    "        ### Begin your code\n",
    "\n",
    "        ### End your code    \n",
    "        \n",
    "    def get_sim_score(self, q, d):\n",
    "        \"\"\" Get the similarity score between a document and a query.\n",
    "        Args:\n",
    "            d (Document) : the document\n",
    "            q (Query) : the query\n",
    "            \n",
    "        Return:\n",
    "            the similarity score\n",
    "        \"\"\"\n",
    "        query_vec = self.get_query_vector(q)\n",
    "        # Define normalizattion functon here or directly pass in normalize_func as shown in below cell\n",
    "        self.doc_weight_scheme['norm'] = self.bm25f_normalize_doc_vec #modified\n",
    "        norm_doc_vec = self.get_doc_vector(q, d, self.doc_weight_scheme) #modified\n",
    "        # Normalization\n",
    "        return self.get_net_score(q, query_vec, d, norm_doc_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bm25f_scorer = BM25FScorer(my_idf, query_dict, default_params_bm25f)\n",
    "\n",
    "# You can directly pass in normalize_func here or define normalizattion functon as shown in above cell\n",
    "query_weight_scheme = {\"tf\": 'b', \"df\": 't', \"norm\": None} \n",
    "doc_weight_scheme = {\"tf\": 'n', \"df\": 'n', \"norm\": bm25f_scorer.bm25f_normalize_doc_vec}\n",
    "\n",
    "\n",
    "print('QUERY Vector: ',  bm25f_scorer.get_query_vector(q, query_weight_scheme), '\\n')\n",
    "print('unnormalized doc vector', bm25f_scorer.get_doc_vector(q, d, None), '\\n')\n",
    "print('score after normalize doc vector', bm25f_scorer.get_sim_score(q, d), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VII Task3: Smallest Window (10%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final task is to incorporate window sizes into the ranking algorithm from Task 2 (or Task 1 if you prefer). For a given query, the smallest window $w_{q,d}$ is defined to be the smallest sequence of tokens in document $d$ such that all of the terms in the query $q$ for are present in that sequence. A window can only be specific to a particular field and for anchor fields, all of the terms in $q$ must be present within a particular anchor text (i.e, if one term occurs in one anchor text and another term in a different anchor text, then it cannot be considered for a window). If $d$ does not contain any of the query terms or a window cannot be found, then $w_{q,d} = \\infty$. Intuitively, the smaller $w_{q,d}$ is, the more relevant the document should be to the query. Thus, we can also multiply the document score (from Task 1 or Task 2) by a boost based on $w$ such that:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* If $w_{q,d} = \\infty$, then the boost is 1. \n",
    "* If $w_{q,d} = |Q|$ where $Q$ are the unique terms in $q$, then we multiply the score by some factor $B$. \n",
    "* For values of $w_{q,d}$ between the query length and infinite, we provide a boost between $B$ and 1. The boost should decrease rapidly with the size of $w_{q,d}$ and can decrease exponentially or as $\\frac{1}{x}$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, for this task, there are either 7 or 15 parameters to optimize, depending on whether you decide to modify cosine similarity or BM25F. The choice of function to use when the window size is not the same as the query length is another factor to also consider. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params depends on the scorer you are using\n",
    "default_params_window = {\n",
    " 'B': 1.16,\n",
    " 'url_weight': 0.1,\n",
    " 'title_weight': 0.1,\n",
    " 'body_hits_weight': 0.1,\n",
    " 'header_weight': 0.1,\n",
    " 'anchor_weight': 0.1,\n",
    " 'b_url': 0.1,\n",
    " 'b_title': 0.1,\n",
    " 'b_header': 0.1,\n",
    " 'b_body_hits': 0.1,\n",
    " 'b_anchor': 0.1,\n",
    " 'k1': 0.1,\n",
    " 'pagerank_lambda': 0.1,\n",
    " 'pagerank_lambda_prime': 0.1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%tee submission/params_window.py\n",
    "# params depends on scorer you are using\n",
    "### Begin your code\n",
    "params_window = {\n",
    "    \n",
    "}\n",
    "### End your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%tee submission/smallest_window_score.py\n",
    "from submission.cosine_score import * #modified\n",
    "from submission.bm25f_score import * #modified\n",
    "class SmallestWindowScorer(BM25FScorer): \n",
    "    \"\"\"\n",
    "     A skeleton for implementing the Smallest Window scorer in Task 3.\n",
    "     Note: The class provided in the skeleton code extends BM25Scorer in Task 2. \n",
    "     However, you don't necessarily have to use Task 2. (You could also use Task 1, \n",
    "     in which case, you'd probably like to extend CosineSimilarityScorer instead.)\n",
    "     Also, feel free to modify or add helpers inside this class.\n",
    "     \n",
    "     Note: If you plan to use cosine similarity scorer\n",
    "             - change parent class to CosineSimilarityScorer \n",
    "             - change normalization method in get_sim_score \n",
    "    \"\"\"\n",
    "    def __init__(self, idf, query_dict, params, query_weight_scheme=None, doc_weight_scheme=None): #modified\n",
    "        super().__init__(idf, query_dict, params, query_weight_scheme=query_weight_scheme, doc_weight_scheme=doc_weight_scheme) #modified\n",
    "        self.query_dict = query_dict\n",
    "        \n",
    "        # smallest window specific weights\n",
    "        self.B = params[\"B\"]\n",
    "    \n",
    "    # Write helper functions here\n",
    "    ### Begin your code\n",
    "\n",
    "    ### End your code\n",
    "        \n",
    "    def get_boost_score(self, q, d):\n",
    "        \"\"\" calculate boost score based on smallest window size\"\"\"\n",
    "        ### Begin your code\n",
    "\n",
    "        ### End your code\n",
    "    \n",
    "\n",
    "    def get_sim_score(self, q, d):\n",
    "        \"\"\" Get the similarity score between a document and a query.\n",
    "        Args:\n",
    "            d (Document) : the document\n",
    "            q (Query) : the query\n",
    "            \n",
    "        Return:\n",
    "            the raw similarity score times boost\n",
    "        \"\"\"\n",
    "        boost = self.get_boost_score(q, d)\n",
    "        query_vec = self.get_query_vector(q)\n",
    "        # Define normalizattion functon here or directly pass in normalize_func as shown in below cell\n",
    "        # Depends on which parent class you are using \n",
    "        self.doc_weight_scheme['norm'] = self.bm25f_normalize_doc_vec #modified\n",
    "        norm_doc_vec = self.get_doc_vector(q, d, self.doc_weight_scheme) #modified\n",
    "        raw_score = self.get_net_score(q, query_vec, d, norm_doc_vec)\n",
    "       \n",
    "        return boost * raw_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smallest_window_scorer = SmallestWindowScorer(my_idf, query_dict, default_params_window)\n",
    "\n",
    "# You can directly define weight_scheme here \n",
    "query_weight_scheme = {\"tf\": 'b', \"df\": 't', \"norm\": None}  \n",
    "doc_weight_scheme = {\"tf\": 'n', \"df\": 'n', \"norm\": smallest_window_scorer.bm25f_normalize_doc_vec}\n",
    "\n",
    "print('QUERY Vector: ',  smallest_window_scorer.get_query_vector(q, query_weight_scheme), '\\n')\n",
    "print('unnormalized doc vector', smallest_window_scorer.get_doc_vector(q, d, None), '\\n')\n",
    "print('score after normalize doc vector', smallest_window_scorer.get_sim_score(q, d), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VIII Rank and Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the rank class, you need to construct the ranking results based on different scores. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%tee submission/rank.py\n",
    "from collections import Counter\n",
    "from collections import OrderedDict\n",
    "\n",
    "class Rank:\n",
    "    def score(self, query_dict, score_type, idf, params):\n",
    "        \n",
    "        \"\"\" Call this function to score and rank documents for some queries, \n",
    "            with a specified scoring function.\n",
    "        Args:\n",
    "            query_dict (dict) :  Mapping of Query-url-Document.\n",
    "            score_type (str) : \"baseline\"  \"cosine\" \"bm25f\" \"window\" \"extra\"\n",
    "            idf (dict) : term-idf dictionary\n",
    "            params(dict) : parames for scorer\n",
    "        Return \n",
    "            query_rankings (dict) : a mapping of queries to rankings\n",
    "        \"\"\"\n",
    "        if score_type == \"baseline\": scorer = BaselineScorer(idf)\n",
    "        elif score_type == \"cosine\": scorer = CosineSimilarityScorer(idf, query_dict, params)\n",
    "        elif score_type == \"bm25f\": scorer = BM25FScorer(idf, query_dict, params)\n",
    "        elif score_type == \"window\": scorer = SmallestWindowScorer(idf, query_dict, params)\n",
    "        elif score_type == \"extra\": scorer = ExtraCreditScorer(idf, query_dict, params) \n",
    "        else: print(\"Wrong score type!\")\n",
    "\n",
    "        # loop through urls for query, getting scores\n",
    "        query_rankings = {}\n",
    "        for query in query_dict.keys():\n",
    "            doc_and_scores = {}\n",
    "            # rank the urls based on scores\n",
    "            ### Begin your code\n",
    "\n",
    "            ### End your code\n",
    "        \n",
    "        return query_rankings\n",
    "    \n",
    "    def rank_with_score(self, input_dict):\n",
    "        \n",
    "        \"\"\" Call this function to accept dictionary with an ordered ranking of queries. \n",
    "        You will need to implement this function for the learning-to-rank ipython notebook. \n",
    "        Note that this function will likely replicate code from the score function above.\n",
    "        Args:\n",
    "            input_dict (dict) :  Mapping of Query-url-score.\n",
    "        Return \n",
    "            query_rankings (dict) : An ordered dictionary of Query->url->score (ordering done for each query)\n",
    "        \n",
    "        \"\"\"\n",
    "        # loop through urls for query, getting scores\n",
    "        query_rankings = {}\n",
    "        for query in input_dict.keys():\n",
    "            url_and_scores = {}\n",
    "            # sort the urls based on scores\n",
    "            ### Begin your code\n",
    "\n",
    "            ### End your code\n",
    "        return query_rankings\n",
    "    \n",
    "    def write_ranking_to_file(self, query_rankings, ranked_result_file):\n",
    "        with open(ranked_result_file, \"w\") as f:\n",
    "            for query in query_rankings.keys():\n",
    "                f.write(\"query: \"+ query.__str__() + \"\\n\")\n",
    "                for res in query_rankings[query]:\n",
    "                \n",
    "                    url_string = \"  url: \" + res.url + \"\\n\" + \\\n",
    "                                \"    title: \" + res.title + \"\\n\" +\\\n",
    "                                \"    debug: \" + \"\\n\" \n",
    "                    \n",
    "                    f.write(url_string)\n",
    "                    \n",
    "        print(\"Write ranking result to \" + ranked_result_file + \" sucessfully!\")     \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write your result to file to check your implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is an example of how to use ranking class \n",
    "# Ranking \n",
    "r = Rank()\n",
    "query_rankings = r.score(query_dict, 'bm25f', my_idf, default_params_bm25f)\n",
    "ranked_result_file = os.path.join(\"output\", \"ranked_result_bm25f\")\n",
    "r.write_ranking_to_file(query_rankings, ranked_result_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VIII.1 NDCG implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We provide the NDCG implementation for you in 'base_classes/ndcg.py'. You can use them to evaluate your results and do paramater tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an example of how to use the Rank class and NDCG class to evaluate your scorer and ranking function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is an example of how to use ranking class and NDCG\n",
    "\n",
    "# Load data and generate query dict\n",
    "signal_file_name = \"pa3.signal.train\"\n",
    "query_dict = load_train_data(os.path.join(data_dir, signal_file_name))\n",
    "\n",
    "# Ranking \n",
    "r = Rank()\n",
    "query_rankings = r.score(query_dict, 'cosine', my_idf, default_params_cosine)\n",
    "ranked_result_file = os.path.join(\"output\", \"ranked_result_cosine\")\n",
    "r.write_ranking_to_file(query_rankings, ranked_result_file)\n",
    "\n",
    "# NDCG\n",
    "ndcg = NDCG()\n",
    "rel_filename = 'pa3.rel.train'\n",
    "rel_file = os.path.join(data_dir, rel_filename)\n",
    "\n",
    "ndcg.get_rel_scores(rel_file)\n",
    "ndcg.read_ranking_calc(ranked_result_file)\n",
    "\n",
    "# You can also write the ndcg result to file\n",
    "ndcg_result_file = os.path.join(\"output\", \"ndcg_result_cosine\") \n",
    "ndcg.write_ndcg_result(ndcg_result_file)\n",
    "\n",
    "# calculate average NDCG\n",
    "avg_ndcg = ndcg.get_avg_ndcg()\n",
    "print(avg_ndcg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure to use NDCG on **both training and development** sets for all tasks to do parameter tuning. \n",
    "\n",
    "\n",
    "\n",
    "Your solution will be evaluated on a hidden test set, and full credit will be given to models that are within 1% of the staff implementation's test-set . Typically, the higher the better. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Report (15%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. (1%) Report NDCG on both training and development sets for all tasks.**\n",
    "> *Your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. (2%)  For the three tasks, you should report all final model parameter values. Describe the intuition when tuning your models, and why those weights work in getting a good score. Were there any particular properties about the documents that allowed a higher weight to be given to one field as opposed to another?**\n",
    "> *Your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. (3%)  In BM25F, in addition to the weights given to the fields, there are 8 other parameters, $B_{url}$, $B_{title}$, $B_{header}$, $B_{body}$, $B_{anchor}$, λ, λ′ and K1. How do these parameters affect the ranking function?**\n",
    "> *Your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. (3%)In task 1, you may either use raw frequencies or sublinearly scale them to compute term frequency. Please report your choice and the reasons behind them. For BM25F, why did you select a particular $V_j$ function?**\n",
    "> *Your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5.(3%)  Briefly describe your design of smallest window. For a function that includes the smallest window as one component, how does varying B and the boost function change the performance of the ranking algorithm?**\n",
    "> *Your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6.(3%)  What other metrics, not used in this assignment, could be used to get a better scoring function from the document? The metrics could either be static (query-independent, e.g. document length) or dynamic (query-dependent, e.g. smallest window).**\n",
    "> *Your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  You are all done in the PA3 part 1. Now, it's time to start part 2 to explore different approaches to learn the parameters for ranking functions using machine learning. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "66px",
    "width": "252px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
